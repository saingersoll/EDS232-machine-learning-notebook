---
title: "Ingersoll_Lab2"
author: "Sofia Ingersoll"
date: '2024-01-24'
output: pdf_document
---

Today we will be continuing the pumpkin case study from last week. We will be using the data that you cleaned and split last time (pumpkins_train) and will be comparing our results today to those you have already obtained. Open and run your Lab 1.Rmd as a first step so those objects are available in your Environment.

Once you have done that, we'll start today's lab by specifying a recipe for a polynomial model.  First we specify a recipe that identifies our variables and data, converts the package variable to a numerical form, and then adds a polynomial effect with step_poly()

```{r include = FALSE}
# load libraries
library(tidymodels)
library(tidyverse)
library(dplyr)
library(janitor)
library(corrplot)
library(lubridate)
library(RColorBrewer)

set.seed(123)

dat <- read_csv(file = "https://raw.githubusercontent.com/MaRo406/eds-232-machine-learning/main/data/pumpkin-data.csv")

# Clean names to the snake_case convention
pumpkins <- dat %>% clean_names(case = "snake")

pumpkins <- pumpkins %>% select(variety, city_name, package, low_price, high_price, date)

# Extract the month and day from the dates and add as new columns
pumpkins <- pumpkins %>%
  mutate(date = mdy(date),  
         day = yday(date),
         month = month(date))
pumpkins %>% 
  select(-day)

# Create a new column price
pumpkins <- pumpkins %>% 
  mutate(price = (low_price + high_price)/2)

# Retain only pumpkins with "bushel" in the package column

new_pumpkins <- pumpkins %>%           # data frame called
                                       # columns of interest to keep
  select(variety, city_name, package, price, date, month, day) %>% 
  filter(
    str_detect(package, 'bushel'))     # remove row observations in that do not
                                       # contain the phrase 'bushel' in the
                                       # package column

# Convert the price if the Package contains fractional bushel values
new_pumpkins <- new_pumpkins %>% 
  mutate(price = case_when(
    str_detect(package, "1 1/9") ~ price/(1.1),
    str_detect(package, "1/2") ~ price*2,
    TRUE ~ price))

# Split the data into training and test sets
pumpkins_split <- new_pumpkins %>% 
  initial_split(prop = 0.8)


# Extract training and test data
pumpkins_train <- training(pumpkins_split)
pumpkins_test <- testing(pumpkins_split)


# Create a recipe for preprocessing the data
lm_pumpkins_recipe <- recipe(price ~ package, data = pumpkins_train) %>% 
  step_integer(all_predictors(), zero_based = TRUE)


# Create a linear model specification
lm_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")


# Hold modeling components in a workflow
lm_wf <- workflow() %>% 
  add_recipe(lm_pumpkins_recipe) %>% 
  add_model(lm_spec)

# Train the model
lm_wf_fit <- lm_wf %>% 
  fit(data = pumpkins_train)


# Make predictions for the test set
predictions <- lm_wf_fit %>% 
  predict(new_data = pumpkins_test)


# Bind predictions to the test set
lm_results <- pumpkins_test %>% 
  select(c(package, price)) %>% 
  bind_cols(predictions)

# Encode package column
package_encode <- lm_pumpkins_recipe %>% 
  prep() %>% 
  bake(new_data = pumpkins_test) %>% 
  select(package)
```

```{r}
# Specify a recipe
poly_pumpkins_recipe <-
  recipe(price ~ package, data = pumpkins_train) %>%
  step_integer(all_predictors(), zero_based = TRUE) %>% 
  step_poly(all_predictors(), degree = 4)
```

How did that work? Later we will learn about model tuning that will let us do things like find the optimal value for degree.  For now, we'd like to have a flexible model, so we'll use a relatively large value.

Polynomial regression is still linear regression, so our model specification looks similar to before.

```{r}
# Create a model specification called poly_spec
poly_spec <- linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression")
```
Question 1: Now take the recipe and model specification that just created and bundle them into a workflow called poly_df.

```{r}
# Bundle recipe and model spec into a workflow
poly_wf <- workflow() %>% 
  add_recipe(poly_pumpkins_recipe) %>% 
  add_model(poly_spec)
```

Question 2: fit a model to the pumpkins_train data using your workflow and assign it to poly_wf_fit
```{r}
# Create a model
poly_wf_fit <- poly_wf %>% 
  fit(data = pumpkins_train)
```

```{r}
# Print learned model coefficients
poly_wf_fit
```

```{r}
# Make price predictions on test data
poly_results <- poly_wf_fit %>% predict(new_data = pumpkins_test) %>% 
  bind_cols(pumpkins_test %>% select(c(package, price))) %>% 
  relocate(.pred, .after = last_col())

# Print the results
poly_results %>% 
  slice_head(n = 10)
```

Now let's evaluate how the model performed on the test_set using yardstick::metrics().
```{r}
metrics(data = poly_results, truth = price, estimate = .pred)
```
Question 3: How do the performance metrics differ between the linear model from last week and the polynomial model we fit today?  Which model performs better on predicting the price of different packages of pumpkins?

Let's visualize our model results.  First prep the results by binding the encoded package variable to them.
```{r}
# Bind encoded package column to the results
poly_results <- poly_results %>% 
  bind_cols(package_encode %>% 
              rename(package_integer = package)) %>% 
  relocate(package_integer, .after = package)


# Print new results data frame
poly_results %>% 
  slice_head(n = 5)
```

OK, now let's take a look! 

Question 4: Create a scatter plot that takes the poly_results and plots package vs. price.  Then draw a line showing our model's predicted values (.pred). Hint: you'll need separate geoms for the data points and the prediction line.
```{r warning=FALSE, message=FALSE}
# Make a scatter plot
  ggplot(poly_results, aes(x = package_integer,
                 y = .pred)) +
  geom_jitter(aes(color = package),
             alpha = .5) +
  geom_line(aes(y = .pred),
            color = 'tan',
            width = 3) +
  labs(title = "Pumpkin Packages vs Price ($)",
       x = 'Package Integer Values',
       y = 'Pumpkin Price ($)') 
```

You can see that a curved line fits your data much better.

Question 5: Now make a smoother line by using geom_smooth instead of geom_line and passing it a polynomial formula like this:
geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = "midnightblue", size = 1.2, se = FALSE)

```{r warning=FALSE, message=FALSE}
# Make a smoother scatter plot 
  ggplot(poly_results, aes(x = package_integer,
                 y = .pred)) +
  geom_jitter(aes(color = package),
             alpha = .5) +
  geom_line(aes(y = .pred),
            color = 'tan',
            width = 4) +
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = 'midnightblue', size = 1.2, se = FALSE) +
  labs(title = "Pumpkin Packages vs Price ($)",
       x = 'Package Integer Values',
       y = 'Pumpkin Price ($)') 
```

OK, now it's your turn to go through the process one more time.
 
Additional assignment components :
6. Choose a new predictor variable (anything not involving package type) in this dataset.
```{r}
# Specify a recipe
poly_var_recipe <-
  recipe(price ~ variety,
         data = pumpkins_train) %>%
  step_integer(all_predictors(),
               zero_based = TRUE)

# encode it for a polynomial model
variety_baked <- poly_var_recipe %>% 
  prep() %>% 
  bake(new_data = pumpkins_test) %>% 
  select(variety)
```

7. Determine its correlation with the outcome variable (price).  (Remember we calculated a correlation matrix last week)

There is a negative correlation of `-0.861` between the variety type and price of pumpkins.
```{r}
# Prep & bake the recipe to determine correlation
poly_baked <- prep(poly_var_recipe) %>% 
  bake(new_data = NULL)

# Bind encoded package column to the results
poly_baked <- poly_baked %>% 
  mutate(variety_integer = as.integer(variety)) %>% 
  relocate(variety_integer, .after = variety)

# determine variety correlation with price
cor(poly_baked$variety_integer, poly_baked$price)

# Obtain correlation matrix
corr_mat <- cor(poly_baked) 

# Make a correlation plot between the variables
corrplot(corr_mat, method = "shade", shade.col = NA, tl.col = "black", tl.srt = 45, addCoef.col = "black", cl.pos = "n", order = "original")
```

8. Create and test a model for your new predictor:
  - Create a recipe
  - Build a model specification (linear or polynomial)
  
```{r}
# Create a recipe 
glm_poly_recipe <- recipe(price ~ variety, 
                         data = pumpkins_train) %>% 
  step_integer(all_predictors(),
               zero_based = TRUE) %>% 
  step_poly(all_predictors(),
            degree = 3)

# Create a model specification called poly_spec
poly_mod_spec <- linear_reg() %>% 
  set_engine("glm") %>% 
  set_mode("regression")
```
  - Bundle the recipe and model specification into a workflow
  - Create a model by fitting the workflow
  
```{r}
# Bundle recipe and model spec into a workflow
poly_var_wf <- workflow() %>% 
  add_recipe(glm_poly_recipe) %>% 
  add_model(poly_mod_spec)

# Create a model
poly_var_wf_fit <- poly_var_wf %>% 
  fit(data = pumpkins_train)
```
 
  - Evaluate model performance on the test data
  
  A large value of `4.13` was tabulated for the estimated RMSE, indicating a greater variance in the data distribution compared to the model fit. The values output by this polynomial model are smaller than the previous linear model. Therefore, we can deduce that we are on track for improving our model. However, we still have a reasonable ways to go before our model will be running more accurately.
  
```{r}
# Let's create price predictions using test data
poly_var_results <- poly_var_wf_fit %>% 
  predict(new_data = pumpkins_test) %>% 
  bind_cols(pumpkins_test %>% 
              select(c(variety, price))) #%>% 
#relocate(variety_integer, .after = variety) 

poly_var_results <- poly_var_results %>% 
  cbind(poly_baked$variety_integer) 


# Let's take a peek at the results
poly_var_results %>% 
  slice_head(n = 10)


# Evaluate performance of polynomial regression
metrics(data = poly_var_results, truth = price, estimate = .pred)
```  
  - Create a visualization of model performance
  
  The visualization below demonstrates how the initial fitting method was overfitting for the variance in the model. Whereas a polynomial regression provided a smoother curve with less precision and more overall accuracy of data distribution when fitting.
  
```{r message=FALSE, warning=FALSE}
# Creating a visualization
  ggplot(poly_var_results, 
         aes(poly_baked$variety_integer,
                 y = price)) +
  geom_jitter(aes(color = variety),
             alpha = .5) +
  geom_line(aes(y = .pred),
            color = 'tan',
            width = 4) +
  geom_smooth(method = lm, formula = y ~ poly(x, degree = 3), color = 'midnightblue', size = 1.2, se = FALSE) +
  labs(title = "Pumpkin Varieties vs Price ($)",
       x = 'Variety Integer Values',
       y = 'Pumpkin Price ($)') 
```
  
Lab 2 due 1/24 at 11:59 PM
