---
title: "Lab 3 Demo"
author: "Sofia Ingersoll"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr)                # good way to assess variables and determine if we need to do transformation
library(glmnet)
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data, this is used in our textbook
dat <- AmesHousing::make_ames()

```

##Train a model
```{r intial_split}
# Data splitting with {rsample} 
set.seed(123)                                # set a seed for reproducibility
split <- initial_split(dat, prop = .8)       # the default is 70/30

# assign different portions of data to training and testing
ames_train <-  training(split)
ames_test  <- testing(split)

```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)
# this function also does auto coding for a new variable automatically

X <- model.matrix(Sale_Price ~ ., data = ames_train)[,-1]      # for sale price, assess the relationship amongst all variables
# There is an intercept variable of -1 that we would like to be removed

# transform y with log() transformation
Y <- log(ames_train$Sale_Price)

# In the Console, run skim() to quickly take a look at the data summary output 

```

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0                   # tells function whether we want lasso or ridge model, 0 = ridge, 1 = lasso, anything in between is Elastic
)

#plot() the glmnet model object
# we are simplifying the model by including this penalty term to the sum of squared errors. This is to remove unncessary coefficients
plot(ridge,
     xvar = 'lambda')  

# As we increase the size of lambda, we can see the coefficients are shrinking towards 0.
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda %>%       # this is what is being output by our model assessment
  head()


# small lambda results in large coefficients
# output in console
coef(ridge)[
  c('Latitude',                        # this is indexing the coefficients for these specific variables
    'Overall_QualVery_Excellent'),
            100]                       # 100th row this is indexing our lambda, as we go down, we get smaller

# what about for small coefficients?
coef(ridge)[
  c('Latitude',
    'Overall_QualVery_Excellent'),
            1]                       # 1th row, this is indexing our lambda, at the top, we're larger and will result in smaller coefficients
  
```
How much improvement to our loss function as lambda changes?

##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
# cross validation for tuning. We are going to be resampling our data by breaking it into pieces and running the multi-folds. The average of this will be our assessment of model efficiency on unseen data.
# the default number of folds is 10-fold

# cv is telling us to use cross validation
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0
)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 1
)
```
  
```{r}
# plot results
par(mfrow = c(1, 2))

# how well the model is predicting the hold out cell, mean-squared error. as lambda increases, the MSE is increasing. We want a lambda that minimizes MSE. This is telling us where our optimal points are for MSE. The dotted lines are created by the one standard error rule. Pick the point where lambda is the most parse-harmonious (fewer variables).This is within 1 std error of the folds. The average mean squared error over the entire cross validation
plot(ridge, main = "Ridge penalty\n\n")


plot(lasso, main = "Lasso penalty\n\n")
```

10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard error"?

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
#-------Ridge model----------------------
# minimum MSE output in console
min(ridge$cvm)

# lambda for this min MSE
# what is the lambda value at the min MSE
ridge$lambda.min

# 1-SE rule
# output a lambda value the is away by the smallest 1-squared error away
ridge$cvm[ridge$lambda == ridge$lambda.1se]


# lambda for this MSE
ridge$lambda.1se


#-------Lasso model---------------------

# minimum MSE
min(lasso$cvm)


# lambda for this min MSE
lasso$lambda.min


# 1-SE rule
# same as above

# lambda for this MSE
lasso$lambda.1se


# No. of coef | 1-SE MSE
# this is to access our feature selection
# numbers that went to zero
lasso$nzero[lasso$lambda == lasso$lambda.min]


# what is the affect of using that standard rule of 1-SE, we reduce the number of coefficients from 144 to 76
lasso$nzero[lasso$lambda == lasso$lambda.1se]

```

```{r}
# Ridge model
ridge_min <- glmnet()

# Lasso model
lasso_min


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "pink", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "plum", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "pink", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "plum", lty = "dashed")
```

