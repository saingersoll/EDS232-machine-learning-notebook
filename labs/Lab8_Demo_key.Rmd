---
title: "Lab 8 Demo"
author: "Mateo Robbins"
date: "2024-03-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(dplyr)
library(kernlab)
```


```{r}
#Create simulate training data for our SVM exercise
set.seed(1)

sim_data <- tibble(
  x1 = rnorm(40),
  x2 = rnorm(40),
  y = factor(rep(c(-1,1),20))) %>%
    mutate(x1 = ifelse(y==1, x1+1.5, x1),
            x2 = ifelse(y==1, x2+1.5, x2))
    
  
sim_data
```

```{r}
#plot to see the structure of the data we created
ggplot(sim_data, aes(x1, x2, color = y)) +
  geom_point()
```

```{r svm_rec}
#specify a recipe where we center to mean of 0 and scale to sd of 1
svm_rec <- recipe(y~., data = sim_data) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

```{r svm_spec}
#Create linear SVM model specification

svm_linear_spec <- svm_poly(degree = 1, cost = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

In SVM, the cost parameter influences the width of the margin around the separating hyperplane. A smaller C allows a wider margin but more misclassifications are allowed. Recall that we can improve  generalization by accepting more errors on the training set. A larger C aims for a narrower margin that tries to correctly classify as many training samples as possible, even if it means a more complex model.

```{r}
#Bundle into workflow
svm_workflow <- workflow() %>%
  add_recipe(svm_rec) %>%
  add_model(svm_linear_spec)
```

```{r}
#Fit workflow
svm_linear_fit <- fit(svm_workflow, data=sim_data)
```

```{r}
#Plot the fit from kernlab engine
svm_linear_fit %>%
  extract_fit_engine() %>%
  plot()
  
```


```{r tune}
#As usual we want to tune our hyperparameter values
svm_linear_wf <- workflow() %>%
  add_model(svm_linear_spec %>%
  set_args(cost_tune())) %>%
  add_formula(y~.)

set.seed(1234)
sim_data_fold <- vfold_cv(sim_data, strata = y)

param_grid <- grid_regular(cost(), levels = 10)

tune_res <- tune_grid(
  svm_linear_wf,
  resamples = sim_data_fold,
  grid = param_grid
)

autoplot(tune_res)
```

#Finalize model and fit
```{r finalize}
best_cost <- select_best(tune_res, metric = "accuracy")

svm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)

svm_linear_fit <- svm_linear_final %>% fit(sim_data)
svm_linear_fit
```


```{r sim_test}
#Create a small test data set
set.seed(2)

sim_data_test <- tibble(
  x1 = rnorm(40),
  x2 = rnorm(40),
  y = factor(rep(c(-1,1),20))) %>%
    mutate(x1 = ifelse(y==1, x1+1.5, x1),
            x2 = ifelse(y==1, x2+1.5, x2))


```

We can use d from {broom} to use our trained model to predict on new data (test data) and add additional info for examining model performance. 

```{r augment}
augment(svm_linear_fit, new_data = sim_data_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

That went well, but makes SVMs really interesting is that we can use non-linear kernels. Let us start by generating some data, but this time generate with a non-linear class boundary.

```{r}
set.seed(3)
sim_data2 <- tibble(
  x1 = rnorm(200) + rep(c(2,-2,0),c(100,50,50)),
  x2 = rnorm(200) +rep(c(2,-2,0), c(100,50,50)),
y= factor(rep(c(1,2), c(150,50)))
)

sim_data2 %>%
  ggplot(aes(x1,x2,color = y)) +
  geom_point()
```

```{r svm_rbf}
svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```


```{r}
#Fit the new specification
svm_rbf_fit <- svm_rbf_spec %>%
  fit(y~., data = sim_data2)

svm_rbf_fit
```

```{r}
#Plot the fit
svm_rbf_fit %>%
  extract_fit_engine() %>%
  plot()
```

```{r}
#Create the test data
set.seed(4)
sim_data2_test <-  tibble(
  x1 = rnorm(200) + rep(c(2,-2,0),c(100,50,50)),
  x2 = rnorm(200) +rep(c(2,-2,0), c(100,50,50)),
y= factor(rep(c(1,2), c(150,50)))
)
```

```{r}
#Examine model performance via confustion matrix
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

ROC Curves

```{r}
#We can examine our model's performance using ROC and AUC
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_curve(truth = y, .pred_1) %>%
  autoplot()

augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_auc(truth = y, .pred_1)
```
