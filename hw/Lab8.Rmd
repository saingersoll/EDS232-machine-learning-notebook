---
title: "Lab 8"
author: "Sofia Ingersoll"
date: "2024-03-13"
output: html_document
---

## Forest Cover Classification with SVM

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict `cover_type` forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.


```{r, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----          System Settings          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidymodels)
library(tidyverse)
library(kernlab)
library(ranger) 
library(here)

set.seed(99)
```

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>

Explore the data.

-   What kinds of features are we working with?

-   Does anything stand out that will affect you modeling choices?

Working with tree coverage data: geolocation details, geochemical categories, & tree cover type. Columns Elevation - Wilderness_Area_Rawah are continuous variables. Soil Type(s) are binary categorical variables, a classification model would best suit this data. Everything is listed as a col_double() attribute.


```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Load Data                                    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# load data
# any factor mutations applied here were overwritten in initial_split()
data <- read_csv(here("data", "covtype_sample.csv"),
                       show_col_types = FALSE) %>% 
  janitor::clean_names()

# commented out for rendering, but inspection of data set
#str(data)
#Ã¥unique(data)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Split Data                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# split data stratified by cover_type
data_split = initial_split(data, 
                           prop = .8, 
                           strata = cover_type) 

# get training data
train = training(data_split) %>% 
  mutate(
    cover_type = as.factor(cover_type)
  )


# get testing data
test = testing(data_split)  %>% 
  mutate(
    cover_type = as.factor(cover_type)
  )


#head(train)
#head(test)
#str(train$cover_type)
#str(test$cover_type)
```

Hint: Pay special attention to the distribution of the outcome variable across the classes.

2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                      Prep Data for Models                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# specify a recipe for data 
recipe <- recipe(cover_type ~., data = train) %>%
   # creates a specification of a recipe step removeS variables that contain only a single value.
  # these cols were causing tuning errors
  step_zv(soil_type_7, soil_type_8, soil_type_15) %>% 
  # create dummy variables from all factors
  step_dummy(all_nominal_predictors()) %>%      
  # normalize all numeric predictors
  # centers and scales the numeric predictors,
  # making step_center() and step_scale() redundant
  step_normalize(all_numeric_predictors())# %>% 
 # step_center(all_predictors()) %>%
 # step_scale(all_predictors())


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         SVM Model Spec                               ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create non-linear SVM model specification
svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Workflow 
#Bundle into workflow
svm_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(svm_rbf_spec)

# Fit model 


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         RF Model Spec                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create non-linear SVM model specification
rf_spec <- rand_forest(mtry = tune(),
                  trees = tune()) %>% 
  # this is a package loaded in libs
  set_engine('ranger') %>% 
  set_mode('classification')

# Workflow 
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(recipe)

# Fit model 
```


3.  Create the folds for cross-validation.

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Cross Val Folds                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create 5 cv folds stratified by outcome score
cv_folds <- vfold_cv(train, 
                     v = 5,
                     )
```


4.  Tune the models. Choose appropriate parameters and grids. If the computational costs of tuning given your strategy are prohibitive, how might you work around this?

```{r}
param_grid <- grid_regular(cost(),
                           levels = 5)

tune_res <- tune_grid(
  svm_linear_wf,
  resamples = sim_data_fold,
  grid = param_grid
)

autoplot(tune_res)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                     RF Cross Validation                              ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# use cross validation to tune mtry and treesfor 5 parameters combinations
# this takes a very long time to run
#in parallel, tune grid params 
doParallel::registerDoParallel() 

system.time(
  rf_cv_tune <- tune_grid(
    rf_workflow,
    resamples = cv_folds,
    grid = 5         
  )
)

```


5.  Conduct final predictions for both models and compare their prediction performances and computation costs from part 4.

-   Which type of model do you think is better for this task?
-   Why do you speculate this is the case?

```{r}

```

