---
title: "Lab 8"
author: "Sofia Ingersoll"
date: "2024-03-13"
output: html_document
---

## Forest Cover Classification with SVM

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict `cover_type` forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.


```{r, echo = TRUE, eval = TRUE, message=FALSE, warning=FALSE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----          System Settings          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(doParallel) 
library(tidymodels)
library(tidyverse)
library(patchwork)
library(kernlab)
library(ranger) 
library(dials)
library(here)
library(vip)
library(gt)

set.seed(99)
```

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>

Explore the data.

-   What kinds of features are we working with?

-   Does anything stand out that will affect you modeling choices?

Working with tree coverage data: geolocation details, geochemical categories, & tree cover type. Columns Elevation - Wilderness_Area_Rawah are continuous variables. Soil Type(s) are binary categorical variables, a classification model would best suit this data. Everything is listed as a col_double() attribute.


```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Load Data                                    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# load data
# any factor mutations applied here were overwritten in initial_split()
data <- read_csv(here("data", "covtype_sample.csv"),
                       show_col_types = FALSE) %>% 
  janitor::clean_names()

# commented out for rendering, but inspection of data set
#str(data)
#Ã¥unique(data)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Split Data                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# split data stratified by cover_type
data_split = initial_split(data, 
                           prop = .8, 
                           strata = cover_type) 

# get training data
train = training(data_split) %>% 
  mutate(
    cover_type = as.factor(cover_type)
  )


# get testing data
test = testing(data_split)  %>% 
  mutate(
    cover_type = as.factor(cover_type)
  )


#head(train)
#head(test)
#str(train$cover_type)
#str(test$cover_type)
```

Hint: Pay special attention to the distribution of the outcome variable across the classes.

2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                      Prep Data for Models                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# specify a recipe for data 
recipe <- recipe(cover_type ~., data = train) %>%
   # creates a specification of a recipe step removeS variables that contain only a single value.
  # these cols were causing tuning errors
  step_zv(soil_type_7, soil_type_8, soil_type_15) %>% 
  # create dummy variables from all factors
  step_dummy(all_nominal_predictors()) %>%      
  # normalize all numeric predictors
  # centers and scales the numeric predictors,
  # making step_center() and step_scale() redundant
  step_normalize(all_numeric_predictors())


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         SVM Model Spec                               ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create non-linear SVM model specification
svm_rbf_spec <- svm_rbf(cost = tune()) %>%
  set_mode("classification") %>%
  set_engine("kernlab")

# Workflow 
#Bundle into workflow
svm_workflow <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(svm_rbf_spec %>% 
  set_args(cost_tune()))


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         RF Model Spec                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create non-linear SVM model specification
rf_spec <- rand_forest(mtry = tune(),
                  trees = tune()) %>% 
  # this is a package loaded in libs
  set_engine('ranger') %>% 
  set_mode('classification')

# Workflow 
rf_workflow <- workflow() %>% 
  add_model(rf_spec) %>% 
  add_recipe(recipe)
```


3.  Create the folds for cross-validation.

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Cross Val Folds                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create 5 cv folds stratified by outcome score
cv_folds <- vfold_cv(train, 
                     v = 5,
                     strata = cover_type
                     )
```


4.  Tune the models. Choose appropriate parameters and grids. If the computational costs of tuning given your strategy are prohibitive, how might you work around this?

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                    SVM Cross Validation                              ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a grid to tune across
param_grid <- grid_regular(cost(),
                           levels = 5)


doParallel::registerDoParallel() 

system.time(
  svm_cv_tune <- tune_grid(
    svm_workflow,
    resamples = cv_folds,
    grid = param_grid
  )
)



#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                     RF Cross Validation                              ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# use cross validation to tune mtry and treesfor 5 parameters combinations
# this takes a very long time to run
#in parallel, tune grid params 
doParallel::registerDoParallel() 

system.time(
  rf_cv_tune <- tune_grid(
    rf_workflow,
    resamples = cv_folds,
    grid = 5         
  )
)

```


5.  Conduct final predictions for both models and compare their prediction performances and computation costs from part 4.

-   Which type of model do you think is better for this task?
-   Why do you speculate this is the case?


### SMV
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Finalize Workflow                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# store the best tuning metrics
svm_best <- show_best(svm_cv_tune, n = 1, metric = "roc_auc")

# finalize workflow
svm_final <-  finalize_workflow(svm_workflow,
                               svm_best)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                             Fit Model                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# similar functions here.

# fit the KNN model to the training set
train_fit_svm <- fit(svm_final, train) 

# get prediction for test 
test_predict_svm = predict(train_fit_svm, 
                          test) %>% 
  # bind to testing column
  bind_cols(test) 

# get testing prediction probabilities
test_predict2_svm = predict(train_fit_svm,
                           test,
                           type = "prob") %>% 
  # bind to testing column
  bind_cols(test) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                          Evaluate Model                              ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# get accuracy of testing prediction
svm_accuracy <- accuracy(test_predict_svm, 
                        truth = cover_type,
                        estimate = .pred_class) 

test_predict_svm_num <- as.numeric(test_predict_svm$cover_type) %>% 
  as.numeric(test_predict_svm$.pred_class)
           
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                        Visual of Fit Model                          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# visualize model

# create confusion matrix
svm_cm <- test_predict_svm %>% 
  conf_mat(truth = cover_type,
           estimate = .pred_class) %>% 
  # plot confusion matrix with heatmap
  autoplot(type = "heatmap") + 
  
  # change theme
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 0,
                                   hjust=1)) +
  #rotate axis labels
  labs(title = "SVM")
```

###  RF
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Finalize Workflow                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# store the best tuning metrics
rf_best <- show_best(rf_cv_tune, n = 1, metric = "roc_auc")

# finalize workflow
rf_final <-  finalize_workflow(rf_workflow,
                               rf_best)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                             Fit Model                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# similar functions here.

# fit the KNN model to the training set
train_fit_rf <- fit(rf_final, train) 

# get prediction for test 
test_predict_rf = predict(train_fit_rf, 
                          test) %>% 
  # bind to testing column
  bind_cols(test) 

# get testing prediction probabilities
test_predict2_rf = predict(train_fit_rf,
                           test,
                           type = "prob") %>% 
  # bind to testing column
  bind_cols(test) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                          Evaluate Model                              ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# get accuracy of testing prediction
rf_accuracy <- accuracy(test_predict_rf, 
                        truth = cover_type,
                        estimate = .pred_class) 

test_predict_rf_num <- as.numeric(test_predict_rf$cover_type) %>% 
  as.numeric(test_predict_rf$.pred_class)
           
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                        Visual of Fit Model                          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# visualize model

# create confusion matrix
rf_cm <- test_predict_rf %>% 
  conf_mat(truth = cover_type,
           estimate = .pred_class) %>% 
  # plot confusion matrix with heatmap
  autoplot(type = "heatmap") + 
  
  # change theme
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 0,
                                   hjust=1)) +
  #rotate axis labels
  labs(title = "Random Forest")
```

### Comparing Predictions of SVM to RF

In the confusion matrix below, a greater number of accurate predictions were made by the random forest model. It is because of this, I think is random forest is better for this task. It took rough 30 seconds longer computationally, which is an acceptable cost.
```{r fig.width=12, fig.height=6}
svm_cm + rf_cm
```

