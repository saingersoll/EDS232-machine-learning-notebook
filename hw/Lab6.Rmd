---
title: "Lab 6"
author: "Sofia Ingersoll"
date: "2023-03-01"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.

## Set Up

```{r echo = TRUE, message = FALSE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         System Set Up                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
knitr::opts_chunk$set(echo = TRUE, message = FALSE#, warning = FALSE
)

library(here)          # easy file paths
library(tidyverse)     # data wrangling
library(tidymodels)    # modeling
library(xgboost)       # package for boosted trees
library(ranger)        # package for random forest non-default external package
library(patchwork)
library(parsnip)
library(doParallel)  # for parallel backend to foreach
library(dials)
library(gt)
library(vip)

set.seed(99)           # set random seed
```

## Data

Grab the training and evaluation data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 


```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Load Data                                    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# read in eel model training data 
train_data <- read_csv(here("data", "eel.model.data.csv"),
                       show_col_types = FALSE) %>% 
  mutate(Angaus = as.factor(Angaus)) %>% 
  mutate(Method = as.factor(Method)) %>% 
  select(-Site)

# read in eel model eval data
eval_dat <- read_csv(here("data", "eel.eval.data.csv"),
                     show_col_types = FALSE) %>% 
  mutate(Angaus = as.factor(Angaus_obs)) %>% 
  mutate(Method = as.factor(Method))
```

### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r split-resample}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Split Data                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# split the data stratified by Angaus
train_split <- initial_split(train_data,
                             strata = Angaus,
                             prop = .8)

# traing data
train <- training(train_split)

# testing data
test <- testing(train_split)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                       Cross Val Folds                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create 10 cv folds stratified by outcome score
cv_folds <- vfold_cv(train, 
                     v = 10,
                     #strata = Angaus
                     )
```

### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r xgb-recipe}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                      Prep Data for Models                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create model recipe
eel_recipe <- recipe(Angaus ~ .,
                     data = train) %>%  
  # create dummy variables from all factors
  step_dummy(all_nominal_predictors()) %>%   
  # normalize all numeric predictors
  step_normalize(all_numeric_predictors())  
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

learn_rate is the rate that the algorithm adapts between each tree iteration

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                     Tuning for Learn Rate                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# creating a boosted tree and tuning for the hyperparam learn_rate
xgb_spec <- boost_tree(learn_rate = tune(),
                       trees = 3000) %>% 
  set_engine('xgboost') %>% 
  set_mode('classification')


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                           Workflow                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create workflow
xgb_workflow <- workflow() %>% # create workflow
  add_model(xgb_spec) %>%     # add boosted trees model
  add_recipe(eel_recipe)      # add recipe

xgb_workflow
```

2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run. You could use {tictoc} or Sys.time().

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                          Tune Grid                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# establish grid to tune learn_rate
xgb_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

# in parallel, tune grid params 
doParallel::registerDoParallel() 

system.time(
  xgb_cv_tune <- tune_grid(
    xgb_workflow,
    resamples = cv_folds,
    grid = xgb_grid        
  )
)
```

3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                     Collect Metrics                                  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#collect_metrics(xgb_cv_tune) # get metrics from tuning cv to pick best model

autoplot(xgb_cv_tune)  +      # plot cv results for parameter tuning
  theme_bw()

best_learn <- show_best(xgb_cv_tune,
                        n = 1,
                        metric = "accuracy") 
# display pretty table
show_best(xgb_cv_tune,
                        n = 1,
                        metric = "accuracy")  %>% gt()
```

### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                           Tuning xgb                                 ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# creating a boosted tree 
xgb_spec_tune <- boost_tree(learn_rate = best_learn,
                            trees = 3000,
                            tree_depth = tune(),
                            min_n = tune(),
                            loss_reduction = tune()) %>% 
  set_engine('xgboost') %>% 
  set_mode('classification')


# extract parameters
xgb_params <- xgb_spec_tune %>% 
  extract_parameter_set_dials()


# create workflow
xgb_workflow_tune <- workflow() %>% 
  add_model(xgb_spec_tune) %>% 
  add_recipe(eel_recipe)

xgb_workflow_tune
```

2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                      Tune LHC Grid                                   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# add lect 6 params
system.time(

xgb_lhc_grid <- xgb_workflow_tune %>% 
  tune_grid(resamples = cv_folds,
            grid = grid_latin_hypercube(x = xgb_params, size = 10))

)

```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                     Collect Metrics                                  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
collect_metrics(xgb_lhc_grid) # get metrics from tuning cv to pick best model

autoplot(xgb_lhc_grid) +      # plot cv results for parameter tuning
  theme_bw()

# display pretty table
 select_best(xgb_lhc_grid,
                        metric = "accuracy") %>% gt()
```

### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                   Prep Data Tuned Params                             ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# isolate best value for tree parameters
best_grid <- select_best(xgb_lhc_grid,
                        metric = "accuracy")
best_min_n <- as.numeric(best_grid$min_n)
best_tree_depth <- as.numeric(best_grid$tree_depth)
best_loss_reduction <- as.numeric(best_grid$loss_reduction)

# create updated specifications
xgb_spec_tuned <- boost_tree(learn_rate = best_learn,
                        trees = 3000,
                        tree_depth = best_tree_depth,
                        min_n = best_min_n,
                        loss_reduction = best_loss_reduction,
                        
                        mtry = tune(),
                        sample_size = tune()) %>% 
  set_engine("xgboost", nthread = 2) %>% 
  set_mode("classification")

# isolate parameters
xgb_param <- xgb_spec_tuned %>% extract_parameter_set_dials()

# create workflow with updated specs
xgb_workflow_tuned <- workflow() %>% 
  add_model(xgb_spec_tuned) %>% 
  add_recipe(eel_recipe)

```


2.  Set up a tuning grid. Use grid_latin_hypercube() again.

```{r}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----               Tune with latin-hyper-cube                             ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

system.time(
  
  xgb_cv_tuned <- xgb_workflow_tuned %>% 
    tune_grid(resamples = cv_folds,
              grid = grid_latin_hypercube(finalize(xgb_param, x = train),
                size = 10))
)
```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
collect_metrics(xgb_cv_tuned) #get metrics from tuning cv to pick best model

autoplot(xgb_cv_tuned) + #plot cv results for parameter tuning
  theme_bw()


best_tune <- select_best(xgb_cv_tuned,
                        metric = "accuracy")
# display pretty table
 select_best(xgb_cv_tuned,
             metric = "accuracy") %>% gt()
```


## Finalize workflow and make final prediction

1.  How well did your model perform? What types of errors did it make?

The model had ~83% accuracy when predicting. 

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                         Finalize Workflow                            ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

xgb_best <- show_best(xgb_cv_tuned, n = 1, metric = "accuracy") # get metrics for best xgb model

                     
xgb_final <-finalize_workflow(xgb_workflow_tuned, select_best(xgb_cv_tuned, metric = "accuracy"))
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                             Fit Model                                ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# fit the model to the training data
xgb_fit <- fit(xgb_final, train)

# make final prediction using test data
eel_predict <- predict(object = xgb_fit, new_data = test) %>% 
  bind_cols(test)

# collect and view metrics
metrics <- as.data.frame(accuracy(eel_predict, truth = Angaus,
                                  estimate = .pred_class))
metrics %>% gt()

```
                     
                     
## Fit your model the evaluation data and compare performance
                     
 1.  Now used your final model to predict on the other dataset (eval.data.csv)

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----                         Predict on Eval Data                          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# make predictions using model and evaluation data
eval_eel_predict <- predict(object = xgb_fit, new_data = eval_dat) %>% 
  bind_cols(eval_dat)


```
                     
                     
2.  How does your model perform on this data?
                       
```{r}
## ==================================================
##               2. Model Performance            ----
## ==================================================
# collect and view metrics
metrics_final <- as.data.frame(accuracy(eval_eel_predict, truth = Angaus,
                                  estimate = .pred_class))
metrics_final %>% gt()
```
                     
                     
3.  How do your results compare to those of Elith et al.?
                       
-   Use {vip} to compare variable importance
 -   What do your variable importance results tell you about the distribution of this eel species?
 
Running the predictions on the evaluation data, a slight decrease in accuracy of about ~1% was observed in thepredictions for the testing data. This is similar to the Elith et al., the two most important factors were summer air temperature (SegSumT) and proportion of area within indigenous forest (USNative). After these two, the variable importance for the model was different than the importance proposed by Elith et al.Â Based on the similarities in variable importance.
                       
```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ----                        Visual of Fit Model                          ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Visualize fit model
xgb_fit %>% 
  vip(geom = 'col',
      aesthetics = list(fill = 'midnightblue')) 
```
                     
                     